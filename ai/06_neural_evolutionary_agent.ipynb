{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c1dceb-fedd-4c84-9b85-06604f100798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8556e472-a9c4-4f69-9e13-dba606b09c84",
   "metadata": {},
   "source": [
    "tensorflow cookbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43218746-9a42-4dec-852e-d09af5eb9b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392de5a6-695d-47f8-a4c8-0fc597770b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain(keras.Model):\n",
    "    def __init__(self, action_dim = 5, input_shape = (1.8 * 8)):\n",
    "        super(Brain, self).__init__()\n",
    "        self.dense1 = layers.Dense(32, input_shape = input_shape, activation = 'relu')\n",
    "        self.logits = layers.Dense(action_dim)\n",
    "    def call(self, inputs):\n",
    "        x = tf.convert_to_tensor(inputs)\n",
    "        logits = self.logits(self.dense1(x))\n",
    "        return logits\n",
    "    def process(self, observations):\n",
    "        action_logits = self.proedict_on_batch(observations)\n",
    "        return action_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345ef33d-bbc7-4251-b216-a42ac87aecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, action_dim = 5, input_shape = (1.8 * 8)):\n",
    "        self.brain = Brain(action_dim, input_shape)\n",
    "        self.brain.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "        self.policy = self.policy_mlp\n",
    "    def policy_mlp(self, observations):\n",
    "        observations = observations.reshape(1, -1)\n",
    "        action_logits = self.brain.process(observations)\n",
    "        action = tf.random.categorical(tf.math.log(action_logits), num_samples = 1)\n",
    "        return action\n",
    "    def get_action(self, observations):\n",
    "        return self.policy(observations)\n",
    "    def learn(self, obs, actions, **kwargs):\n",
    "        self.brain.fit(obs, actions, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85aa86d-57b9-4b15-aead-647535d011cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trajectory = namedtuple('Trajectory', ['obs', 'actions', 'reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258b0386-fc16-4e4c-b922-e57193f5109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, render = True):\n",
    "    obs, episode_reward, done, step_num, info = env.reset(), .0, False, 0, None\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        obs,reward,done,info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        step_num += 1\n",
    "        if render:\n",
    "            env.render()\n",
    "    return step_num, episode_reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba913d8-197b-4786-aa89-62f96fc9d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(agent, env, render=False):\n",
    "    obs, episode_reward, done, step_num - env.reset(), .0, False, 0\n",
    "    observations, actions = [], []\n",
    "    episode_reward = .0\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, done, info = env.step(action)\n",
    "        observations.append(np.array(obs).reshape(-1))\n",
    "        actions.append(np.squeeze(action, 0))\n",
    "        episode_reward += reward\n",
    "        obs = next_obs\n",
    "        step_num += 1\n",
    "        if render:\n",
    "            env.render()\n",
    "    env.close()\n",
    "    return observations, actions, episode_reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
